---
sidebar_position: 6
title: Hallucinations
---

In the context of Generative AI and Large Language Models, hallucinations
refer to the phenomenon in which a model generates outputs that are fluent
and plausible but factually incorrect, misleading, or unsupported by reliable
evidence.
These outputs are not random errors, they are coherent responses produced
by the model based on learned statistical patterns rather than verified
knowledge.

## Causes of hallucinations
Hallucinations arise from structural characteristics of LLMs:

1. **Probabilistic generation:**
    LLMs generate text by predicting the most likely next token, not by
    validating facts. As a results, they prioritize linguistic coherence over
    factual accuracy.
2. **Training data limitations**:
    Models learn from imperfect, incomplete, and sometimes contradictory
    data sources, which can introduce inaccuracies.
3. **Lack of real-time knowledge**:
    Most LLMs do not have native access to live databases or external
    verification systems during generation.
4. **Ambiguous or incomplete prompts**:
    Vague or underspecified inputs can lead the model to “fill in gaps” with
    fabricated information.
5. **Overgeneralization**:
    Models may apply learned patterns to context where they are not valid.

## Types of hallucinations

| Type | Description | Example |
|------|-------------|---------|
| **Factual hallucination** | Incorrect dates, names, statistics or events | "The Eiffel Tower was built in 1867" (actually 1889) |
| **Source hallucinations** | Invented citations or references | Citing a paper with DOI 10.1234/fake.2023 that doesn't exist |
| **Logical hallucination** | Internally inconsistent reasoning | "All cats are mammals. Dogs are cats. Therefore, dogs are mammals." |
| **Contextual hallucination** | Misinterpretation of user intent | User asks about Python (programming) but receives info about pythons (snakes) |
| **Fabricated content** | Creation of non-existent entities or concepts | Inventing a historical figure or scientific discovery |

Contextual hallucination misinterpretation of user intent
Fabricated content Creation of non-existent entities or
concepts

**Note:** This represents a small, non-exhaustive set of examples.

While there are a number of methods to identify hallucinations (such as
human review, fact-checking, automated verification systems etc), fully
automated detection remains an open research challenge.

# Mitigation strategies

1. **Retrieval-augmented generation (RAG):**
    Integrates external knowledge sources to ground model responses in
    verified data.
2. **Fine-tuning and alignment:** Improves reliability through more high
    quality and curated data and human feedback.
3. **Prompt engineering** : Carefully structured prompts reduce ambiguity
    and guide accurate responses.
4. **Confidence calibration** : Encouraging model to express uncertainty
    when appropriate.
5. **Human-in-the-loop system** : Maintaining human oversight for critical
    outputs
6. **Multi-model verification** : Cross-checking outputs across different
    models to identify inconsistencies.
7. **Temperature and sampling parameters** : Lower temperature settings
    reduce randomness and creative divergence.
Related concepts
RAG
Fine–tuning
prompt engineering
Gen AI
LLMs


